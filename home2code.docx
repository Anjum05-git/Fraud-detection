import pandas as pd
import numpy as np
import plotly.express as px
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score,precision_score,recall_score, classification_report, confusion_matrix
import missingno as msno
from sklearn.preprocessing import OneHotEncoder
pd.set_option('display.max_columns', None) 
pd.set_option('display.max_rows', None)
import warnings
warnings.simplefilter("ignore")
Train = pd.read_csv(r"C:\Users\abhay\OneDrive\Desktop\ipd2\Train-1542865627584.csv")
Train_Inpatientdata=pd.read_csv(r"C:\Users\abhay\OneDrive\Desktop\ipd2\Train_Inpatientdata-1542865627584.csv")
Train_Outpatientdata=pd.read_csv(r"C:\Users\abhay\OneDrive\Desktop\ipd2\Train_Outpatientdata-1542865627584.csv")
Train_Beneficiarydata=pd.read_csv(r"C:\Users\abhay\OneDrive\Desktop\ipd2\Train_Beneficiarydata-1542865627584.csv")

# Load Test Dataset
Test = pd.read_csv(r"C:\Users\abhay\OneDrive\Desktop\ipd2\Test-1542969243754.csv")
Test_Beneficiarydata=pd.read_csv(r"C:\Users\abhay\OneDrive\Desktop\ipd2\Test_Beneficiarydata-1542969243754.csv")
Test_Inpatientdata=pd.read_csv(r"C:\Users\abhay\OneDrive\Desktop\ipd2\Test_Inpatientdata-1542969243754.csv")
Test_Outpatientdata=pd.read_csv(r"C:\Users\abhay\OneDrive\Desktop\ipd2\Test_Outpatientdata-1542969243754.csv")
print("The Training inpatient data has {} records and {} fields. \n" .format(Train_Inpatientdata.shape[0], Train_Inpatientdata.shape[1]))
print("The Training outpatient data has {} records and {} fields. \n" .format(Train_Outpatientdata.shape[0], Train_Outpatientdata.shape[1]))
print("The Training Benficiary data has {} records and {} fields. \n" .format(Train_Beneficiarydata.shape[0], Train_Beneficiarydata.shape[1]))

print("The Test inpatient data has {} records and {} fields. \n" .format(Test_Inpatientdata.shape[0], Test_Inpatientdata.shape[1]))
print("The Test outpatient data has {} records and {} fields. \n" .format(Test_Outpatientdata.shape[0], Test_Outpatientdata.shape[1]))
print("The Test Benficiary data has {} records and {} fields. \n" .format(Test_Beneficiarydata.shape[0], Test_Beneficiarydata.shape[1]))
Train_Beneficiarydata.head()
print("The Beneficiary Train Data has {} records and {} fields. \n" .format(Train_Beneficiarydata.shape[0], Train_Beneficiarydata.shape[1]))
print("The Beneficiary Test Data has {} records and {} fields. \n" .format(Test_Beneficiarydata.shape[0], Test_Beneficiarydata.shape[1]))

Train_Inpatientdata.isna().sum()
print('There are '+ str(Train_Beneficiarydata.duplicated().sum())+ ' duplicate  in the beneficiary training dataset.')
print('There are '+ str(Test_Beneficiarydata.duplicated().sum())+ ' duplicate in the beneficiary test dataset.')

print('They are', Train_Beneficiarydata['BeneID'].nunique(), 'unique beneficiaries in the training data set')
print('They are', Test_Beneficiarydata['BeneID'].nunique(), 'unique beneficiaries in the test data set')

Train_Beneficiarydata['DOB'] = pd.to_datetime(Train_Beneficiarydata['DOB'], format='%Y-%m-%d')
Train_Beneficiarydata['DOD'] = pd.to_datetime(Train_Beneficiarydata['DOD'], format='%Y-%m-%d', errors='ignore')
Test_Beneficiarydata['DOB'] = pd.to_datetime(Test_Beneficiarydata['DOB'], format="%y-%m-%d",errors='coerce')
Test_Beneficiarydata['DOD'] = pd.to_datetime(Test_Beneficiarydata['DOD'], format='%Y-%m-%d', errors='ignore')

# Calculate age in years and round to the nearest integer
Train_Beneficiarydata['Age'] = round((Train_Beneficiarydata['DOD'] - Train_Beneficiarydata['DOB']).dt.days / 365)
Test_Beneficiarydata['Age'] = round((Test_Beneficiarydata['DOD'] - Test_Beneficiarydata['DOB']).dt.days / 365)

# The final DOD date is '2009-12-01' . We can use that to calculate the ages of those with no DOD

# Convert 'DOB' to datetime format
Train_Beneficiarydata['DOB'] = pd.to_datetime(Train_Beneficiarydata['DOB'], format='%Y-%m-%d')
Test_Beneficiarydata['DOB'] = pd.to_datetime(Test_Beneficiarydata['DOB'], format='%Y-%m-%d')

# Calculate age for beneficiaries with missing 'Age' values, assuming 2009-12-01 as the reference date
Train_Beneficiarydata['Age'].fillna(
    round((pd.to_datetime('2009-12-01') - Train_Beneficiarydata['DOB']).dt.days / 365),
    inplace=True
)
Test_Beneficiarydata['Age'].fillna(
    round((pd.to_datetime('2009-12-01') - Test_Beneficiarydata['DOB']).dt.days / 365),
    inplace=True
)

# Create a new 'AliveorDead' column and flag deceased beneficiaries
Train_Beneficiarydata['AliveorDead'] = Train_Beneficiarydata['DOD'].notna().astype(int)
Test_Beneficiarydata['AliveorDead'] = Test_Beneficiarydata['DOD'].notna().astype(int)

Train_Beneficiarydata.groupby(['AliveorDead'])['BeneID'].nunique()

Train_Inpatientdata.head()

#Lets check missing values in each column in inpatient data
Train_Inpatientdata.isna().sum()

# Calculate number of days patient was admitted from AdmissionDt and DischargeDt

# Convert 'AdmissionDt' and 'DischargeDt' to datetime format
Train_Inpatientdata['AdmissionDt'] = pd.to_datetime(Train_Inpatientdata['AdmissionDt'], format='%Y-%m-%d')
Train_Inpatientdata['DischargeDt'] = pd.to_datetime(Train_Inpatientdata['DischargeDt'], format='%Y-%m-%d')
Test_Inpatientdata['AdmissionDt'] = pd.to_datetime(Test_Inpatientdata['AdmissionDt'], format='%Y-%m-%d')
Test_Inpatientdata['DischargeDt'] = pd.to_datetime(Test_Inpatientdata['DischargeDt'], format='%Y-%m-%d')

# Calculate the number of days of admission, adding 1 to include both the admission and discharge days
Train_Inpatientdata['NumberofDaysAdmitted'] = (Train_Inpatientdata['DischargeDt'] - Train_Inpatientdata['AdmissionDt']).dt.days + 1
Test_Inpatientdata['NumberofDaysAdmitted'] = (Test_Inpatientdata['DischargeDt'] - Test_Inpatientdata['AdmissionDt']).dt.days + 1
# Convert 'ClaimStartDt' and 'ClaimEndDt' to datetime format
Train_Inpatientdata['ClaimEndDt'] = pd.to_datetime(Train_Inpatientdata['ClaimEndDt'], format='%Y-%m-%d')
Train_Inpatientdata['ClaimStartDt'] = pd.to_datetime(Train_Inpatientdata['ClaimStartDt'], format='%Y-%m-%d')

Test_Inpatientdata['ClaimEndDt'] = pd.to_datetime(Test_Inpatientdata['ClaimEndDt'], format='%Y-%m-%d')
Test_Inpatientdata['ClaimStartDt'] = pd.to_datetime(Test_Inpatientdata['ClaimStartDt'], format='%Y-%m-%d')

# Duration of Claim
Train_Inpatientdata['DurationofClaim'] = (Train_Inpatientdata['ClaimEndDt'] - Train_Inpatientdata['ClaimStartDt']).dt.days
Test_Inpatientdata['DurationofClaim'] = (Test_Inpatientdata['ClaimEndDt'] - Test_Inpatientdata['ClaimStartDt']).dt.days

# Add column admitted

Train_Inpatientdata['Admitted'] =1
Test_Inpatientdata['Admitted'] =1

# Distribution of beneficiaries based on Gender
px.histogram(Train_Inpatientdata, x = 'NumberofDaysAdmitted', title='NumberofDaysAdmitted')

# Distribution of beneficiaries based on Gender
px.box(Train_Inpatientdata, x = 'NumberofDaysAdmitted')

# Distribution of beneficiaries based on Gender
px.histogram(Train_Inpatientdata, x = 'DurationofClaim', title='DurationofClaim')

px.histogram(Train_Inpatientdata, x='InscClaimAmtReimbursed', title='InscClaimAmtReimbursed')

Train_Outpatientdata.head()

# Check for duplicate rows 
print('Duplicate rows in train set', Train_Outpatientdata.duplicated().sum())
print('Duplicate rows in test set', Test_Outpatientdata.duplicated().sum())

Train_Outpatientdata.isnull().sum()

# Add column admitted

Train_Outpatientdata['Admitted'] = 0
Test_Outpatientdata['Admitted'] = 0

# Convert 'ClaimStartDt' and 'ClaimEndDt' to datetime format
Train_Outpatientdata['ClaimEndDt'] = pd.to_datetime(Train_Outpatientdata['ClaimEndDt'], format='%Y-%m-%d')
Train_Outpatientdata['ClaimStartDt'] = pd.to_datetime(Train_Outpatientdata['ClaimStartDt'], format='%Y-%m-%d')

Test_Outpatientdata['ClaimEndDt'] = pd.to_datetime(Test_Outpatientdata['ClaimEndDt'], format='%Y-%m-%d')
Test_Outpatientdata['ClaimStartDt'] = pd.to_datetime(Test_Outpatientdata['ClaimStartDt'], format='%Y-%m-%d')

# Duration of Claim
Train_Outpatientdata['DurationofClaim'] = (Train_Outpatientdata['ClaimEndDt'] - Train_Outpatientdata['ClaimStartDt']).dt.days
Test_Outpatientdata['DurationofClaim'] = (Test_Outpatientdata['ClaimEndDt'] - Test_Outpatientdata['ClaimStartDt']).dt.days

#Identify common columns between Outpatient and inpatient data

common_cols = list(set(Train_Inpatientdata.columns).intersection(set(Train_Outpatientdata.columns)))
print(common_cols)

# Merge the DataFrames on the common columns, using an outer join

Train_Allpatientdata = pd.merge(Train_Outpatientdata, Train_Inpatientdata, on=common_cols, how='outer')
Test_Allpatientdata = pd.merge(Test_Outpatientdata, Test_Inpatientdata, on=common_cols, how='outer')

print(Train_Allpatientdata.shape)
print(Test_Allpatientdata.shape)

df_train = Train_Allpatientdata.merge(Train_Beneficiarydata, on='BeneID', how='inner')

df_test = Test_Allpatientdata.merge(Test_Beneficiarydata, on='BeneID', how='inner')

# shape of dataset
print('Training data shape: ', df_train.shape)
print('Test data shape: ', df_test.shape)

# Lets merge patient data with fradulent providers details data with "Provider" as joining key for inner join

df_train1 = pd.merge(Train, df_train,on='Provider')

df_test1 = pd.merge(Test, df_test,on='Provider')

# this value indicates whether the beneficiary has renal disease
df_train1['RenalDiseaseIndicator'].replace('Y','1',inplace=True)
df_train1['RenalDiseaseIndicator'] = df_train1['RenalDiseaseIndicator'].astype(int)
    
    
df_test1['RenalDiseaseIndicator'].replace('Y','1',inplace=True)
df_test1['RenalDiseaseIndicator'] = df_test1['RenalDiseaseIndicator'].astype(int)


# Drop DOD and DOB

df_train1.drop(columns=['DOB', 'DOD'], axis=1, inplace=True)
df_test1.drop(columns=['DOB', 'DOD'], axis=1, inplace=True)

# ClaimDiagnoseIndex, ClmProcedureIndex, ChronicCondIndex

df_train1['ClmDiagnosisCodeIndex'] = df_train1.filter(regex='ClmDiagnosisCode_').notnull().sum(axis=1)
df_test1['ClmDiagnosisCodeIndex'] = df_test1.filter(regex='ClmDiagnosisCode_').notnull().sum(axis=1)

df_train1['ClmProcedureCodeIndex'] = df_train1.filter(regex='ClmProcedureCode_').notnull().sum(axis=1)
df_test1['ClmProcedureCodeIndex'] = df_test1.filter(regex='ClmProcedureCode_').notnull().sum(axis=1)

# drop all features starting 

columns_to_drop = df_train1.filter(regex='ClmProcedureCode_|ClmDiagnosisCode_').columns
df_train1 = df_train1.drop(columns_to_drop, axis=1)
df_test1 = df_test1.drop(columns_to_drop, axis=1)

# fill nan for NumberofDaysAdmitted with 0

df_train1['NumberofDaysAdmitted'] = df_train1['NumberofDaysAdmitted'].fillna(0)
df_test1['NumberofDaysAdmitted'] = df_test1['NumberofDaysAdmitted'].fillna(0)

df_train1 = df_train1.dropna(subset=['AttendingPhysician'])
df_test1 = df_test1.dropna(subset=['AttendingPhysician'])

# Fill missing values with mean

df_train1['DeductibleAmtPaid'] = df_train1['DeductibleAmtPaid'].fillna(df_train1['DeductibleAmtPaid'].mean())
df_test1['DeductibleAmtPaid'] = df_test1['DeductibleAmtPaid'].fillna(df_test1['DeductibleAmtPaid'].mean())

# . Average features grouped by Provider

columns_to_transform = ["InscClaimAmtReimbursed", "DeductibleAmtPaid", "IPAnnualReimbursementAmt", "IPAnnualDeductibleAmt",
    "OPAnnualReimbursementAmt", "OPAnnualDeductibleAmt", "Age", "NoOfMonths_PartACov", "NoOfMonths_PartBCov","DurationofClaim",
                        "NumberofDaysAdmitted"
]

for column in columns_to_transform:
    df_train1[f"PerProviderAvg_{column}"] = df_train1.groupby('Provider')[column].transform('mean')
    df_test1[f"PerProviderAvg_{column}"] = df_test1.groupby('Provider')[column].transform('mean')
    
    
# Average features grouped by BeneID
# Average features grouped by Operating Physician


columns_to_transform = [
    "InscClaimAmtReimbursed",
    "DeductibleAmtPaid",
    "IPAnnualReimbursementAmt",
    "IPAnnualDeductibleAmt",
    "OPAnnualReimbursementAmt",
    "OPAnnualDeductibleAmt", 
    "DurationofClaim",
    "NumberofDaysAdmitted"
    
]


for column in columns_to_transform:
    df_train1[f"PerBeneIDAvg_{column}"] = df_train1.groupby('BeneID')[column].transform('mean')
    df_test1[f"PerBeneIDAvg_{column}"] = df_test1.groupby('BeneID')[column].transform('mean')
    
    df_train1[f"PerAttendingPhysician Avg_{column}"] = df_train1.groupby('AttendingPhysician')[column].transform('mean')
    df_test1[f"PerAttendingPhysician Avg_{column}"] = df_test1.groupby('AttendingPhysician')[column].transform('mean')


# Drop features not needed for model training
df_train1.drop(columns=['ClmAdmitDiagnosisCode', 'Provider', 'State', 'Race', 'Gender', 'County', 'AdmissionDt', 'AttendingPhysician', 'OtherPhysician', 'OperatingPhysician',  
                        'DischargeDt', 'ClaimID', 'ClaimEndDt', 'DiagnosisGroupCode', 'ClaimStartDt', 'BeneID', 'ClaimID'], axis=1, inplace=True)

df_test1.drop(columns=['ClmAdmitDiagnosisCode', 'State', 'Race', 'County', 'Gender', 'AdmissionDt', 'DiagnosisGroupCode', 'OperatingPhysician', 'DischargeDt', 'AttendingPhysician', 'OtherPhysician', 
                       'ClaimID', 'ClaimEndDt', 'ClaimStartDt', 'ClaimID'], axis=1, inplace=True)


# histogram of Fraud_reported

px.histogram(df_train1, x="PotentialFraud",color='PotentialFraud',title="PotentialFraud", height=500, width=700)

# histogram of Admitted

px.histogram(df_train1, x="Admitted",color='PotentialFraud',title="Admitted", height=500, width=700)

# histogram of Fraud_reported

px.histogram(df_train1, x="RenalDiseaseIndicator",color='PotentialFraud',title="RenalDiseaseIndicator", height=500, width=700)

#ClmProcedureCodeIndex

px.histogram(df_train1, x="ClmProcedureCodeIndex",color='PotentialFraud',title="ChronicDiseaseIndex", height=500, width=700)

#ClmDiagnosisCodeIndex

px.histogram(df_train1, x="ClmDiagnosisCodeIndex",color='PotentialFraud',title="ChronicDiseaseIndex", height=500, width=700)

 #convert target to numerical values (0, 1)

df_train1['PotentialFraud'].replace({'No':0, 'Yes': 1}, inplace=True)

# Split data into train and validation set

df_train2 , df_val = train_test_split(df_train1, test_size=0.10, random_state=42)

y_train = df_train2.pop('PotentialFraud')
X_train = df_train2

y_val = df_val.pop('PotentialFraud')
X_val = df_val

X_test = df_test1


print(X_train.shape, X_val.shape, X_test.shape)

# One hot encode ChronicCond_ features

categorical_cols = [col for col in X_train.columns if col.startswith('ChronicCond_')]

encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')

encoded_data_train = encoder.fit_transform(X_train[categorical_cols])
encoded_data_test = encoder.fit_transform(X_test[categorical_cols])
encoded_data_val = encoder.fit_transform(X_val[categorical_cols])


encoded_df_train = pd.DataFrame(encoded_data_train, columns=encoder.get_feature_names_out())
encoded_df_test = pd.DataFrame(encoded_data_test, columns=encoder.get_feature_names_out())
encoded_df_val = pd.DataFrame(encoded_data_val, columns=encoder.get_feature_names_out())

X_train = X_train.reset_index(drop=True)
X_test = X_test.reset_index(drop=True)
X_val = X_val.reset_index(drop=True)



X_train = pd.concat([X_train.drop(categorical_cols, axis=1), encoded_df_train], axis=1)
X_test = pd.concat([X_test.drop(categorical_cols, axis=1), encoded_df_test], axis=1)
X_val = pd.concat([X_val.drop(categorical_cols, axis=1), encoded_df_val], axis=1)

# Treating imbalance data in training dataset

from imblearn.over_sampling import SMOTE

from collections import Counter

counter = Counter(y_train)

print('before smoting: ', counter)

smt = SMOTE()

X_train, y_train = smt.fit_resample(X_train, y_train)

counter = Counter(y_train)
print('After smoting: ', counter)

RF= RandomForestClassifier(n_estimators=100, random_state=0, max_depth=15)


# Fitting model to train set

RF.fit( X_train, y_train) 

# Checking for overfitting and underfitting

print(" Accuracy on training set: ",  RF.score( X_train, y_train)) 

print(" Accuracy on test set: ", RF.score( X_val, y_val))

# Checking for overfitting and underfitting

print(" Accuracy on training set: ",  RF.score( X_train, y_train)) 

print(" Accuracy on test set: ", RF.score( X_val, y_val))

# Prediction on test set

y_pred = RF.predict(X_val)

from sklearn.metrics import recall_score, precision_score, accuracy_score, f1_score

rec = recall_score(y_val, y_pred)
pre = precision_score(y_val, y_pred)
acc = accuracy_score(y_val, y_pred)
f1_sc =  f1_score(y_val, y_pred)

print("Accuracy :: ",acc)
print("Precision :: ",pre)
print("Recall :: ", rec)
print("f1_score", f1_sc)

# Confusion matrix

cm = confusion_matrix(y_val, y_pred)

# Heatmap

plt.figure(figsize=(8, 5))
sns.heatmap(cm, cmap= 'Blues', linecolor='black', fmt='', annot=True)
plt.title('confusion matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# ROC Curve, AUC

from sklearn import metrics

y_pred_proba = RF.predict_proba(X_val)[::,1]
fpr, tpr, _ = metrics.roc_curve(y_val,  y_pred_proba)
auc = metrics.roc_auc_score(y_val, y_pred_proba)
print('auc: ', auc)

#create ROC curve
plt.plot(fpr,tpr,label="AUC="+str(auc))
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.title('ROC')
plt.legend(loc=4)
plt.show()

# calssification report

print(classification_report(y_val, y_pred))

# visualizing the most important features

plt.figure(figsize=(15, 5))
importances = RF.feature_importances_
feature_importance = pd.Series(importances, index = X_train.columns)
feature_importance.plot(kind='bar')
plt.title('Feature importance')
plt.show()




app1.py
before changes
import gradio as gr
import pandas as pd
import joblib

model = joblib.load("model.pkl")

def predict_from_csv(file):
    df = pd.read_csv(file.name)
    preds = model.predict(df)
    df['Prediction'] = preds
    return df

interface = gr.Interface(
    fn=predict_from_csv,
    inputs=gr.File(label="Upload CSV"),
    outputs=gr.Dataframe(label="Prediction Results")
)

interface.launch()